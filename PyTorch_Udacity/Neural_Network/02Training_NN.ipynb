{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Neural Networks**\n",
        "The network we build in the previous part isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilites. The power of neural networks is that we can train them to approximate the function,and basically any function given enough data and compute time.\n",
        "At first the network is naive, it doesn't know the function maping the inputs to the ouputs. We train the network by showing it example of real data, then adjusting the network parameters such that it approximates the function.\n",
        "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a loss function (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems.\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "y8inaX7pLRqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where n is the number of training examples, yi are the true label and y^i are the predicted labels.\n",
        "\n",
        "By minimizing this loss with respect to the network parameters, we can find configuration where the loss is at a minimum and the network is ale to predict the correct labels with high accuracy. We find this minimum using a process called gradient descent. The gradient is the slope of the loss function and points in the direction of faster change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
        "\n",
        "# **Backpropagation**\n",
        "For single layer networks, gradinet descent in straightforward to implement. However it's more complicated for deeper, multiplayer neural networks like the one we've built. Complicated enough that it look about 30 years before researchers figured out how to train multilayer networks.\n",
        "\n",
        "Training multiplayer networks is done through backpropagation which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation"
      ],
      "metadata": {
        "id": "gTXdTUFEfNs6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka4qGvpZLMCu",
        "outputId": "cf14b018-73c5-4b1d-ce89-50d42271bd71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.56MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 57.2kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.26MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.34MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a feed-forward network\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10))\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Get our data\n",
        "images, labels = next(iter(trainloader))\n",
        "# Flatten images\n",
        "images = images.view(images.shape[0], -1)\n",
        "\n",
        "# Forward pass, get our logits\n",
        "logits = model(images)\n",
        "# calculate the loss with the logits and the labels\n",
        "loss = criterion(logits, labels)\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpDnzucpS3e6",
        "outputId": "a6ff14d2-3441-454a-d1a0-731e9090da1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3113, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a detailed explanation of every part of the code:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Model Definition**\n",
        "\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 128),  # Input layer\n",
        "    nn.ReLU(),           # Activation function for first hidden layer\n",
        "    nn.Linear(128, 64),  # Hidden layer\n",
        "    nn.ReLU(),           # Activation function for second hidden layer\n",
        "    nn.Linear(64, 10)    # Output layer\n",
        ")\n",
        "```\n",
        "\n",
        "#### **Components:**\n",
        "\n",
        "1. **`nn.Sequential`:**\n",
        "   - A container for stacking layers in sequence.\n",
        "   - Automatically connects the output of one layer to the input of the next.\n",
        "\n",
        "2. **`nn.Linear(784, 128)`:**\n",
        "   - Fully connected layer.\n",
        "   - Takes an input of size **784** (28x28 pixels flattened into a vector).\n",
        "   - Outputs **128** features for the first hidden layer.\n",
        "\n",
        "3. **`nn.ReLU()`:**\n",
        "   - Activation function: **Rectified Linear Unit**.\n",
        "   - Applies the function \\( f(x) = \\max(0, x) \\) element-wise, introducing non-linearity to the model.\n",
        "   - Helps the network learn more complex patterns.\n",
        "\n",
        "4. **`nn.Linear(128, 64)` and `nn.Linear(64, 10)`**:\n",
        "   - Additional fully connected layers.\n",
        "   - The last layer outputs **10 logits**, corresponding to the 10 classes (digits 0-9).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Loss Function**\n",
        "\n",
        "```python\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "```\n",
        "\n",
        "#### **Components:**\n",
        "\n",
        "- **`nn.CrossEntropyLoss`:**\n",
        "  - Combines:\n",
        "    1. **LogSoftmax:** Converts raw scores (logits) to log-probabilities:\n",
        "       \\[\n",
        "       \\text{log-softmax}(x_i) = \\log \\left( \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} \\right)\n",
        "       \\]\n",
        "    2. **Negative Log Likelihood (NLL):** Measures the difference between predicted log-probabilities and the true labels.\n",
        "\n",
        "#### **Why Use CrossEntropyLoss?**\n",
        "- It directly works with raw logits and automatically applies the softmax transformation, so you don’t need to include a softmax layer in your model.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Getting the Data**\n",
        "\n",
        "```python\n",
        "images, labels = next(iter(trainloader))\n",
        "```\n",
        "\n",
        "#### **What It Does:**\n",
        "- **`iter(trainloader)`:**\n",
        "  - Converts `trainloader` into an iterator to fetch batches of data.\n",
        "- **`next()`:**\n",
        "  - Fetches the next batch (images and labels) from the iterator.\n",
        "\n",
        "#### **What Are `images` and `labels`?**\n",
        "- **`images`:**\n",
        "  - A tensor of shape `[batch_size, 1, 28, 28]`.\n",
        "  - Contains grayscale images of size \\( 28 \\times 28 \\) pixels.\n",
        "- **`labels`:**\n",
        "  - A tensor of shape `[batch_size]`.\n",
        "  - Contains the ground truth labels for the batch (digits 0-9).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Flattening Images**\n",
        "\n",
        "```python\n",
        "images = images.view(images.shape[0], -1)\n",
        "```\n",
        "\n",
        "#### **What It Does:**\n",
        "- Reshapes the tensor to `[batch_size, 784]`.\n",
        "  - **`images.shape[0]`**: Keeps the batch size as it is.\n",
        "  - **`-1`**: Flattens the remaining dimensions into a single vector (28x28 = 784).\n",
        "\n",
        "#### **Why Flatten?**\n",
        "- Fully connected layers (`nn.Linear`) expect 2D input: `[batch_size, input_features]`.\n",
        "- Since images are initially 3D tensors (`[batch_size, 1, 28, 28]`), they need to be flattened.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Forward Pass**\n",
        "\n",
        "```python\n",
        "logits = model(images)\n",
        "```\n",
        "\n",
        "#### **What Happens Here:**\n",
        "1. The `images` tensor (input) is passed through the `model`.\n",
        "2. Each layer processes the input sequentially:\n",
        "   - Input layer → ReLU → Hidden layer → ReLU → Output layer.\n",
        "3. The final output (`logits`) is a tensor of shape `[batch_size, 10]`.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Calculate the Loss**\n",
        "\n",
        "```python\n",
        "loss = criterion(logits, labels)\n",
        "```\n",
        "\n",
        "#### **Components:**\n",
        "- **`logits`:**\n",
        "  - The raw scores (not probabilities) for each of the 10 classes.\n",
        "- **`labels`:**\n",
        "  - Ground truth labels for the batch.\n",
        "\n",
        "#### **How It Works:**\n",
        "1. `criterion` first applies **softmax** to `logits` to convert them into probabilities.\n",
        "2. Then, it compares the predicted probabilities with the true labels to compute the loss.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Print the Loss**\n",
        "\n",
        "```python\n",
        "print(loss)\n",
        "```\n",
        "\n",
        "#### **Output:**\n",
        "- A single scalar value representing how far the predictions are from the true labels.\n",
        "- Example:\n",
        "  ```\n",
        "  tensor(2.3678, grad_fn=<NllLossBackward>)\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **End-to-End Explanation:**\n",
        "\n",
        "1. **Model Initialization:**\n",
        "   - A neural network is defined with 3 fully connected layers and ReLU activations.\n",
        "2. **Data Preparation:**\n",
        "   - A batch of images and labels is fetched and flattened.\n",
        "3. **Forward Pass:**\n",
        "   - Images are passed through the network to compute raw scores (`logits`).\n",
        "4. **Loss Calculation:**\n",
        "   - The loss is computed using `CrossEntropyLoss`, which compares logits with true labels."
      ],
      "metadata": {
        "id": "cfXVGlvzbOTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my experience it's more convenient to build the model with a log-softmax output using `nn.LogSoftmax` or `F.log_softmax`. THe you can get the actual probabilities by taking the exponential `torch.exp(output)`. With a log-softmax output, you want to use the negative log livelhood loss, `nn.NLLoss`."
      ],
      "metadata": {
        "id": "FB3NQcgnbdeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **practice**"
      ],
      "metadata": {
        "id": "5Tekc4D8dM9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim = 1))\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.003)\n",
        "\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images, labels in trainloader:\n",
        "    # Flatten MNIST images into a 784 long vector\n",
        "    images = images.view(images.shape[0], -1)\n",
        "\n",
        "    # TODO: Training pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model.forward(images)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  else:\n",
        "    print(f\"Training loss: {running_loss/len(trainloader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58Vl2kqbY6L9",
        "outputId": "a586c66e-dcda-4173-ccf3-468f7aef74e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 1.788484054841975\n",
            "Training loss: 0.7680185044180355\n",
            "Training loss: 0.5040657781302802\n",
            "Training loss: 0.4235450773160341\n",
            "Training loss: 0.3843182680576341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s an explanation of the full code for training a neural network on the MNIST dataset:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Model Definition**\n",
        "\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 128),  # Input layer\n",
        "    nn.ReLU(),           # First hidden layer with ReLU activation\n",
        "    nn.Linear(128, 64),  # Second hidden layer\n",
        "    nn.ReLU(),           # Second hidden layer activation\n",
        "    nn.Linear(64, 10),   # Output layer for 10 classes\n",
        "    nn.LogSoftmax(dim=1) # LogSoftmax activation for class probabilities\n",
        ")\n",
        "```\n",
        "\n",
        "- **`nn.LogSoftmax(dim=1)`**:\n",
        "  - Converts the logits (raw scores) into log probabilities along the class dimension (`dim=1`).\n",
        "  - The log probabilities are used by the `NLLLoss` function.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Loss Function and Optimizer**\n",
        "\n",
        "```python\n",
        "criterion = nn.NLLLoss()\n",
        "```\n",
        "\n",
        "- **`nn.NLLLoss`**:\n",
        "  - Stands for **Negative Log Likelihood Loss**.\n",
        "  - Works with log probabilities (output from `LogSoftmax`).\n",
        "  - Compares predicted probabilities with the actual labels.\n",
        "\n",
        "```python\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.003)\n",
        "```\n",
        "\n",
        "- **`torch.optim.SGD`**:\n",
        "  - Implements **Stochastic Gradient Descent** optimization.\n",
        "  - `model.parameters()`: Specifies which parameters (weights and biases) to optimize.\n",
        "  - `lr=0.003`: Sets the learning rate, which controls how much the weights are updated during each step.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Training Loop**\n",
        "\n",
        "#### **Epochs**\n",
        "```python\n",
        "epochs = 5\n",
        "```\n",
        "- Defines the number of times the entire dataset is passed through the network.\n",
        "\n",
        "#### **For Loop**\n",
        "```python\n",
        "for e in range(epochs):\n",
        "```\n",
        "- Iterates over the number of epochs.\n",
        "\n",
        "#### **Initialize Running Loss**\n",
        "```python\n",
        "running_loss = 0\n",
        "```\n",
        "- Tracks the cumulative loss for the epoch to calculate the average training loss.\n",
        "\n",
        "#### **Iterating Through Batches**\n",
        "```python\n",
        "for images, labels in trainloader:\n",
        "```\n",
        "- Loops through each batch of images and labels from the `trainloader`.\n",
        "\n",
        "#### **Flatten Images**\n",
        "```python\n",
        "images = images.view(images.shape[0], -1)\n",
        "```\n",
        "- Converts images from \\( [\\text{batch size}, 1, 28, 28] \\) to \\( [\\text{batch size}, 784] \\).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Training Pass**\n",
        "\n",
        "#### **Zero Gradients**\n",
        "```python\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "- Clears the gradients from the previous step to avoid accumulation.\n",
        "\n",
        "#### **Forward Pass**\n",
        "```python\n",
        "output = model.forward(images)\n",
        "```\n",
        "- Passes the input images through the model to compute predictions.\n",
        "\n",
        "#### **Calculate Loss**\n",
        "```python\n",
        "loss = criterion(output, labels)\n",
        "```\n",
        "- Compares the predicted probabilities (`output`) with the actual labels using `NLLLoss`.\n",
        "\n",
        "#### **Backpropagation**\n",
        "```python\n",
        "loss.backward()\n",
        "```\n",
        "- Computes gradients for each parameter (weights and biases) with respect to the loss.\n",
        "\n",
        "#### **Optimize Parameters**\n",
        "```python\n",
        "optimizer.step()\n",
        "```\n",
        "- Updates the model's parameters using the gradients and the learning rate.\n",
        "\n",
        "#### **Track Running Loss**\n",
        "```python\n",
        "running_loss += loss.item()\n",
        "```\n",
        "- Adds the scalar value of the loss to the cumulative running loss.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Print Training Loss**\n",
        "\n",
        "```python\n",
        "else:\n",
        "    print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
        "```\n",
        "\n",
        "- **`running_loss/len(trainloader)`**:\n",
        "  - Divides the cumulative loss by the number of batches to calculate the average training loss for the epoch.\n",
        "\n",
        "---\n",
        "\n",
        "### **Output Example**\n",
        "After running the code, you might see:\n",
        "\n",
        "```plaintext\n",
        "Training loss: 0.5634\n",
        "Training loss: 0.4322\n",
        "Training loss: 0.3768\n",
        "Training loss: 0.3412\n",
        "Training loss: 0.3125\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **How the Model Learns**\n",
        "1. **Forward Pass:**\n",
        "   - The input data is passed through the model to make predictions.\n",
        "2. **Loss Calculation:**\n",
        "   - The loss measures how far the predictions are from the actual labels.\n",
        "3. **Backward Pass:**\n",
        "   - Gradients are computed using backpropagation to adjust the model's parameters.\n",
        "4. **Optimization:**\n",
        "   - The optimizer updates the parameters to minimize the loss."
      ],
      "metadata": {
        "id": "VWUAeeHrn9l8"
      }
    }
  ]
}